# dataflow/dataflowagent/toolkits/pipeline_assembler.py
from __future__ import annotations

import ast
import json
import itertools
from pathlib import Path
from typing import Any, Dict, List, Tuple, DefaultDict
import requests
from dataflow_agent.state import DFState,DFRequest
import importlib
import inspect
import re
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List, Tuple

from dataflow_agent.logger import get_logger
from dataflow.utils.registry import OPERATOR_REGISTRY

log = get_logger(__name__)

EXTRA_IMPORTS: set[str] = set()  

# "pipeline_assembler",        # 核心入口：返回 {"pipe_code": ...}
# "build_pipeline_code",       # 主体：组装 pipeline 代码
# "choose_prompt_template_by_llm", # LLM智能选择 prompt 模板
# "render_operator_blocks",    # 生成 operator 初始化与调用代码
# "group_imports",             # 汇总依赖导入
# "extract_op_params",         # 提取 operator 参数
# "choose_prompt_template",    # prompt_template 兜底选择

def call_llm_for_selection(
    system_prompt: str,
    user_message: str,
    api_url: str,
    api_key: str,
    model: str,
    temperature: float = 0.3,
    max_tokens: int = 100
) -> str:
    """
    调用 LLM API 进行选择决策
    
    Args:
        system_prompt: 系统提示词
        user_message: 用户消息
        api_url: API 地址（OpenAI 兼容格式）
        api_key: API 密钥
        model: 模型名称
        temperature: 温度参数
        max_tokens: 最大 token 数
    
    Returns:
        LLM 返回的文本内容
    """
    if not api_url.endswith('/chat/completions'):
        if api_url.endswith('/'):
            api_url = api_url + 'chat/completions'
        else:
            api_url = api_url + '/chat/completions'
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        result = response.json()
        
        # 提取返回的内容
        content = result.get('choices', [{}])[0].get('message', {}).get('content', '').strip()
        log.info(f"[pipeline_assembler] LLM selection result: {content}")
        return content
        
    except Exception as e:
        log.error(f"[pipeline_assembler] LLM API call failed: {e}")
        raise


def extract_prompt_info(prompt_cls: type) -> Dict[str, Any]:
    """
    提取 prompt 类的详细信息，包括示例提示词
    
    Args:
        prompt_cls: Prompt 类对象
    
    Returns:
        包含类名、模块、文档字符串和示例提示词的字典
    """
    prompt_info = {
        'class_name': prompt_cls.__qualname__,
        'module': prompt_cls.__module__,
        'docstring': (prompt_cls.__doc__ or '').strip(),
    }
    
    # 尝试实例化并获取示例提示词
    try:
        instance = prompt_cls()
        
        # 如果有 build_prompt 方法
        if hasattr(instance, 'build_prompt'):
            sig = inspect.signature(instance.build_prompt)
            params = list(sig.parameters.keys())
            
            # 构造示例参数
            example_args = {}
            for param in params:
                if param == 'self':
                    continue
                # 使用占位符
                example_args[param] = f"<example_{param}>"
            
            try:
                # 调用 build_prompt 获取完整的提示词模板
                example_prompt = instance.build_prompt(**example_args)
                # 截取前 800 字符避免过长
                prompt_info['full_prompt_template'] = example_prompt[:800]
                if len(example_prompt) > 800:
                    prompt_info['full_prompt_template'] += "\n...[truncated]"
            except Exception as e:
                log.warning(f"[pipeline_assembler] Failed to get example prompt for {prompt_cls.__name__}: {e}")
                prompt_info['full_prompt_template'] = "Unable to generate example"
        
        # 如果有其他可用的属性，也可以提取
        if hasattr(instance, 'template'):
            prompt_info['template_attr'] = str(instance.template)[:200]
            
    except Exception as e:
        log.warning(f"[pipeline_assembler] Failed to instantiate {prompt_cls.__name__}: {e}")
        prompt_info['full_prompt_template'] = "Unable to instantiate"
    
    return prompt_info


def choose_prompt_template_by_llm(op_name: str, state: DFState) -> str:
    """
    通过 LLM 选择最合适的 prompt_template
    
    规则：
      1. 提取 operator 的所有 ALLOWED_PROMPTS 候选
      2. 获取每个 prompt 的详细信息（包括提示词模板）
      3. 调用 LLM 让它根据 target 任务描述选择最合适的 prompt
      4. 返回选中 prompt 的实例化代码字符串
    
    Args:
        op_name: Operator 名称
        state: DFState 对象，包含 request.target 等信息
    
    Returns:
        选中的 prompt_template 实例化代码字符串
    """
    cls = OPERATOR_REGISTRY.get(op_name)
    if cls is None:
        raise KeyError(f"Operator {op_name} not found in registry")
    
    # 如果没有 ALLOWED_PROMPTS 或为空，回退到原逻辑
    allowed_prompts = getattr(cls, "ALLOWED_PROMPTS", None)
    if not allowed_prompts:
        log.info(f"[pipeline_assembler] No ALLOWED_PROMPTS for {op_name}, using default logic")
        return choose_prompt_template(op_name, state)
    
    # 如果只有一个候选，直接使用
    if len(allowed_prompts) == 1:
        prompt_cls = allowed_prompts[0]
        EXTRA_IMPORTS.add(f"from {prompt_cls.__module__} import {prompt_cls.__qualname__}")
        return f"{prompt_cls.__qualname__}()"
    
    # 收集所有候选 prompt 的详细信息
    log.info(f"[pipeline_assembler] Extracting info from {len(allowed_prompts)} prompt candidates")
    prompt_candidates = []
    for prompt_cls in allowed_prompts:
        prompt_info = extract_prompt_info(prompt_cls)
        prompt_candidates.append(prompt_info)
    
    # 构造 LLM 请求
    target = state.request.target
    system_prompt = """You are an expert at selecting the most appropriate prompt template for a given task.

Your job is to:
1. Analyze the target task description
2. Review all available prompt templates (including their documentation and example prompts)
3. Select the MOST suitable prompt template

IMPORTANT: 
1.Respond with ONLY the exact class name of the selected prompt template, nothing else.
2. 禁止返回 `Diy开头的` 这个类名，无论如何都不要选择它。
"""
    
    user_message = f"""Target Task Description:
{target}

Available Prompt Templates:
"""
    
    for i, p in enumerate(prompt_candidates, 1):
        user_message += f"\n{'='*60}\n"
        user_message += f"Option {i}: {p['class_name']}\n"
        user_message += f"{'='*60}\n"
        
        if p['docstring']:
            user_message += f"Documentation:\n{p['docstring']}\n\n"
        
        if 'full_prompt_template' in p:
            user_message += f"Prompt Template Example:\n{p['full_prompt_template']}\n"
        
        if 'template_attr' in p:
            user_message += f"Template: {p['template_attr']}\n"
    
    user_message += f"\n{'='*60}\n"
    user_message += "\nBased on the target task, which prompt template is most suitable?\n"
    user_message += "Respond with ONLY the class name (e.g., 'MathAnswerGeneratorPrompt')."
    
    # 调用 LLM
    try:
        selected_class_name = call_llm_for_selection(
            system_prompt=system_prompt,
            user_message=user_message,
            api_url=state.request.chat_api_url,
            api_key=state.request.api_key,
            model=state.request.model
        )
        
        # 清理返回结果（移除可能的引号、空格等）
        selected_class_name = selected_class_name.strip().strip('"\'`')
        
        # 找到对应的 prompt class
        for prompt_cls in allowed_prompts:
            if prompt_cls.__qualname__ == selected_class_name or prompt_cls.__name__ == selected_class_name:
                log.critical(f"[pipeline_assembler] 大模型选择了这个提示词模板: {prompt_cls.__qualname__}")
                EXTRA_IMPORTS.add(f"from {prompt_cls.__module__} import {prompt_cls.__qualname__}")
                return f"{prompt_cls.__qualname__}()"
        
        # 如果没找到精确匹配，尝试模糊匹配
        for prompt_cls in allowed_prompts:
            if selected_class_name in prompt_cls.__qualname__ or prompt_cls.__name__ in selected_class_name:
                log.warning(f"[pipeline_assembler] Using fuzzy match for '{selected_class_name}' -> {prompt_cls.__qualname__}")
                EXTRA_IMPORTS.add(f"from {prompt_cls.__module__} import {prompt_cls.__qualname__}")
                return f"{prompt_cls.__qualname__}()"
        
        # 如果还是没找到，使用第一个作为默认
        log.warning(f"[pipeline_assembler] LLM selected unknown prompt '{selected_class_name}', using first available")
        
    except Exception as e:
        log.error(f"[pipeline_assembler] LLM selection failed: {e}, using first available prompt")
    
    # 默认使用第一个
    prompt_cls = allowed_prompts[0]
    EXTRA_IMPORTS.add(f"from {prompt_cls.__module__} import {prompt_cls.__qualname__}")
    return f"{prompt_cls.__qualname__}()"


# ==================================================================================================================================
def snake_case(name: str) -> str:
    """
    Convert CamelCase (with acronyms) to snake_case.
    Examples:
        SQLGenerator -> sql_generator
        HTTPRequest -> http_request
    """
    s1 = re.sub(r"(.)([A-Z][a-z]+)", r"\1_\2", name)
    s2 = re.sub(r"([a-z0-9])([A-Z])", r"\1_\2", s1)
    return s2.replace("__", "_").lower()


def try_import(module_path: str) -> bool:
    try:
        importlib.import_module(module_path)
        return True
    except Exception as e:
        log.warning(f"[pipeline_assembler] import {module_path} failed: {e}")
        return False


def build_stub(cls_name: str, module_path: str) -> str:
    return (
        f"# Fallback stub for {cls_name}, original module '{module_path}' not found\n"
        f"class {cls_name}:  # type: ignore\n"
        f"    def __init__(self, *args, **kwargs):\n"
        f"        import warnings; warnings.warn(\n"
        f"            \"Stub operator {cls_name} used, module '{module_path}' missing.\"\n"
        f"        )\n"
        f"    def run(self, *args, **kwargs):\n"
        f"        return kwargs.get(\"storage\")  # 透传\n"
    )


def _normalize_module(mod: str) -> str:
    """
    将类似
        dataflow.operators.general_text.eval.langkit_sample_evaluator
    统一裁剪成
        dataflow.operators.general_text

    规则：
        1. 仅处理以 "dataflow.operators." 开头的模块。
        2. 只保留 "dataflow.operators.<一级子包>"。
        3. 其余模块原样返回。
    """
    prefix = "dataflow.operators."
    if mod.startswith(prefix):
        # 拿掉前缀后按点分割，取第 0 个就是一级子包
        subpkg = mod[len(prefix):].split(".", 1)[0]
        return f"{prefix}{subpkg}"
    return mod

def group_imports(op_names: List[str]) -> Tuple[List[str], List[str], Dict[str, type]]:
    imports, stubs = [], []
    op_classes: Dict[str, type] = {}
    module2names: Dict[str, List[str]] = defaultdict(list)

    for name in op_names:
        cls = OPERATOR_REGISTRY.get(name)
        if cls is None:
            raise KeyError(f"Operator <{name}> not in OPERATOR_REGISTRY")
        op_classes[name] = cls

        mod_raw = cls.__module__                       # e.g. dataflow.operators.general_text.eval.langkit_sample_evaluator
        mod = _normalize_module(mod_raw)               # → dataflow.operators.general_text

        if try_import(mod):
            module2names[mod].append(cls.__name__)
        else:                                          # 正常情况下进不到这里
            stubs.append(build_stub(cls.__name__, mod))

    # 只保留一次循环
    for m in sorted(module2names.keys()):
        uniq_names = sorted(set(module2names[m]))
        imports.append(f"from {m} import {', '.join(uniq_names)}")

    # 追加由 choose_prompt_template 收集的 import
    imports.extend(sorted(EXTRA_IMPORTS))
    return imports, stubs, op_classes


def _format_default(val: Any) -> str:
    """
    Produce a code string for a default value.
    If default is missing (inspect._empty), we return 'None' to keep code runnable.
    """
    if val is inspect._empty:
        return "None"
    if isinstance(val, str):
        return repr(val)
    return repr(val)


def extract_op_params(cls: type) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]], bool]:
    """
    Inspect 'cls' for __init__ and run signatures.

    Returns:
        init_kwargs: list of (param_name, code_str_default) for __init__ (excluding self)
        run_kwargs: list of (param_name, code_str_default) for run (excluding self and storage)
        run_has_storage: whether run(...) has 'storage' parameter
    """
    # ---- __init__
    init_kwargs: List[Tuple[str, str]] = []
    try:
        init_sig = inspect.signature(cls.__init__)
        for p in list(init_sig.parameters.values())[1:]:  # skip self
            if p.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
                continue
            init_kwargs.append((p.name, _format_default(p.default)))
    except Exception as e:
        log.warning(f"[pipeline_assembler] inspect __init__ of {cls.__name__} failed: {e}")

    # ---- run
    run_kwargs: List[Tuple[str, str]] = []
    run_has_storage = False
    if hasattr(cls, "run"):
        try:
            run_sig = inspect.signature(cls.run)
            params = list(run_sig.parameters.values())[1:]  # skip self
            for p in params:
                if p.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
                    continue
                if p.name == "storage":
                    run_has_storage = True
                    continue
                run_kwargs.append((p.name, _format_default(p.default)))
        except Exception as e:
            log.warning(f"[pipeline_assembler] inspect run of {cls.__name__} failed: {e}")

    return init_kwargs, run_kwargs, run_has_storage

def choose_prompt_template(op_name: str, state: DFState) -> str:
    """
    返回 prompt_template 的代码字符串。
    规则：
      1. 若类有 ALLOWED_PROMPTS 且非空 → 取第一个并实例化；
      2. 否则回退到 __init__ 默认值；若仍不可用则返回 None。
    """
    from dataflow.utils.registry import OPERATOR_REGISTRY
    import inspect, json

    cls = OPERATOR_REGISTRY.get(op_name)
    if cls is None:
        raise KeyError(f"Operator {op_name} not found in registry")

    # 优先使用 ALLOWED_PROMPTS
    if getattr(cls, "ALLOWED_PROMPTS", None):
        prompt_cls = cls.ALLOWED_PROMPTS[0]
        EXTRA_IMPORTS.add(f"from {prompt_cls.__module__} import {prompt_cls.__qualname__}")
        return f"{prompt_cls.__qualname__}()"

    # -------- 无 ALLOWED_PROMPTS，兜底处理 --------
    sig = inspect.signature(cls.__init__)
    p = sig.parameters.get("prompt_template")
    if p is None:
        # 理论上不会走到这里，因为调用方只在存在该参数时才进来
        return "None"

    default_val = p.default
    if default_val in (inspect._empty, None):
        return "None"

    # 基础类型可直接 repr
    if isinstance(default_val, (str, int, float, bool)):
        return repr(default_val)

    # 类型对象 → 加 import 然后实例化
    if isinstance(default_val, type):
        EXTRA_IMPORTS.add(f"from {default_val.__module__} import {default_val.__qualname__}")
        return f"{default_val.__qualname__}()"

    # UnionType / 其它复杂对象 → 字符串化再 repr，保证可写入代码
    return repr(str(default_val))


def render_operator_blocks(
    op_names: List[str], 
    op_classes: Dict[str, type], 
    state: DFState,
    prompted_generator_prompts: Dict[int, str] = None
) -> Tuple[str, str]:
    """
    Render operator initialization lines and forward-run lines without leading indentation.
    Indentation will be applied by build_pipeline_code when inserting into the template.
    
    Args:
        op_names: 算子名称列表
        op_classes: 算子类字典
        state: DFState 对象
        prompted_generator_prompts: 预生成的 PromptedGenerator system_prompt 映射
            格式: {算子索引: system_prompt}
    """
    init_lines: List[str] = []
    forward_lines: List[str] = []
    prompted_generator_prompts = prompted_generator_prompts or {}

    # 用于跟踪每个算子名称的出现次数
    name_count: Dict[str, int] = {}

    for i, name in enumerate(op_names):
        cls = op_classes[name]
        base_var_name = snake_case(cls.__name__)
        
        # 统计相同算子名称的出现次数
        count = name_count.get(base_var_name, 0) + 1
        name_count[base_var_name] = count
        
        # 如果出现多次，添加序号后缀
        if count > 1:
            var_name = f"{base_var_name}_{count}"
        else:
            var_name = base_var_name

        init_kwargs, run_kwargs, run_has_storage = extract_op_params(cls)

        # Inject pipeline context where appropriate
        rendered_init_args: List[str] = []
        for k, v in init_kwargs:
            if k == "llm_serving":
                rendered_init_args.append(f"{k}=self.llm_serving")
            elif k == "prompt_template":
                # p_t = choose_prompt_template(name, state)
                # 用LLM来选择
                p_t = choose_prompt_template_by_llm(name, state)
                rendered_init_args.append(f'{k}={p_t}')
            elif k == "system_prompt":
                # 检查是否是 PromptedGenerator 且有预生成的 prompt
                if name == "PromptedGenerator" and i in prompted_generator_prompts:
                    # 使用预生成的 system_prompt
                    pre_prompt = prompted_generator_prompts[i]
                    rendered_init_args.append(f'{k}={repr(pre_prompt)}')
                    log.info(f"[render_operator_blocks] 使用预生成的 prompt 给索引 {i} 的 PromptedGenerator")
                else:
                    rendered_init_args.append(f"{k}={v}")
            else:
                rendered_init_args.append(f"{k}={v}")

        init_line = f"self.{var_name} = {cls.__name__}(" + ", ".join(rendered_init_args) + ")"
        init_lines.append(init_line)

        # Build run call
        run_args: List[str] = []
        if run_has_storage:
            run_args.append("storage=self.storage.step()")
        run_args.extend([f"{k}={v}" for k, v in run_kwargs])

        if run_args:
            call = (
                f"self.{var_name}.run(\n"
                f"    " + ", ".join(run_args) + "\n"
                f")"
            )
        else:
            call = f"self.{var_name}.run()"
        forward_lines.append(call)

    return "\n".join(init_lines), "\n".join(forward_lines)


def indent_block(code: str, spaces: int) -> str:
    """
    Indent every line of 'code' by 'spaces' spaces. Keeps internal structure.
    """
    import textwrap as _tw
    code = _tw.dedent(code or "").strip("\n")
    if not code:
        return ""
    prefix = " " * spaces
    return "\n".join(prefix + line if line else "" for line in code.splitlines())


def write_pipeline_file(
    code: str,
    file_name: str = "recommend_pipeline.py",
    overwrite: bool = True,
) -> Path:
    """
    把生成的 pipeline 代码写入当前文件同级目录下的 `file_name`。
    """
    target_path = Path(__file__).resolve().parent / file_name

    if target_path.exists() and not overwrite:
        raise FileExistsError(f"{target_path} already exists. Set overwrite=True to replace it.")

    target_path.write_text(code, encoding="utf-8")
    log.info(f"[pipeline_assembler] code written to {target_path}")

    return target_path

# =========================================================渲染 op 的 全部函数==================================================
# def snake_case(name: str) -> str:
#     """CamelCase -> snake_case"""
#     s1 = re.sub(r"(.)([A-Z][a-z]+)", r"\1_\2", name)
#     s2 = re.sub(r"([a-z0-9])([A-Z])", r"\1_\2", s1)
#     return s2.replace("__", "_").lower()


# def try_import(module_path: str) -> bool:
#     """尝试导入模块"""
#     try:
#         importlib.import_module(module_path)
#         return True
#     except Exception as e:
#         log.warning(f"import {module_path} failed: {e}")
#         return False


# def _normalize_module(mod: str) -> str:
#     """dataflow.operators.xxx.yyy.zzz -> dataflow.operators.xxx"""
#     prefix = "dataflow.operators."
#     if mod.startswith(prefix):
#         subpkg = mod[len(prefix):].split(".", 1)[0]
#         return f"{prefix}{subpkg}"
#     return mod


def group_import_for_full_params(op_names: List[str]) -> tuple:
    """收集所有算子的导入语句"""
    imports = []
    op_classes: Dict[str, type] = {}
    module2names: Dict[str, List[str]] = defaultdict(list)

    for name in op_names:
        cls = OPERATOR_REGISTRY.get(name)
        if cls is None:
            raise KeyError(f"Operator <{name}> not in OPERATOR_REGISTRY")
        op_classes[name] = cls

        mod_raw = cls.__module__
        mod = _normalize_module(mod_raw)

        if try_import(mod):
            module2names[mod].append(cls.__name__)

    for m in sorted(module2names.keys()):
        uniq_names = sorted(set(module2names[m]))
        imports.append(f"from {m} import {', '.join(uniq_names)}")

    # 追加额外的 import
    # imports.extend(sorted(EXTRA_IMPORTS))
    return imports, op_classes

# def render_operator_blocks_with_full_params(
#     opname_and_params: List[Dict[str, Any]], 
#     op_classes: Dict[str, type]
# ) -> tuple:
#     """
#     渲染算子初始化和调用代码（完整支持 init + run 参数）
    
#     Args:
#         opname_and_params: [
#             {
#                 "op_name": "OperatorA",
#                 "init_params": {"llm_serving": "...", "prompt_template": "..."},
#                 "run_params": {"param1": "value1"}
#             },
#             ...
#         ]
    
#     Returns:
#         (init_code_block, forward_code_block)
#     """
#     import inspect
    
#     init_lines = []
#     forward_lines = []
#     # 记录同一算子类出现的次数，避免 self.xxx 被后面的覆盖
#     name_count: Dict[str, int] = {}

#     for item in opname_and_params:
#         name = item["op_name"]
#         custom_init_params = item.get("init_params", {})
#         custom_run_params = item.get("run_params", {})
        
#         cls = op_classes[name]
#         base_var_name = snake_case(cls.__name__)
#         count = name_count.get(base_var_name, 0) + 1
#         name_count[base_var_name] = count
#         if count > 1:
#             var_name = f"{base_var_name}_{count}"
#         else:
#             var_name = base_var_name

#         # 检查 run 方法是否有 storage 参数
#         run_has_storage = False
#         if hasattr(cls, "run"):
#             try:
#                 run_sig = inspect.signature(cls.run)
#                 run_has_storage = "storage" in run_sig.parameters
#             except:
#                 pass

#         # -------- 渲染 __init__ 参数 --------
#         init_args = []
#         for k, v in custom_init_params.items():
#             if k == "llm_serving":
#                 init_args.append(f"{k}=self.llm_serving")
#             elif k == "prompt_template":
#                 # 用户已经选择了具体的 prompt 类
#                 if v and v != "None":
#                     # v 格式：module.ClassName
#                     parts = v.rsplit(".", 1)
#                     if len(parts) == 2:
#                         module, classname = parts
#                         EXTRA_IMPORTS.add(f"from {module} import {classname}")
#                         init_args.append(f"{k}={classname}()")
#                     else:
#                         init_args.append(f"{k}={v}")
#                 else:
#                     init_args.append(f"{k}=None")
#             else:
#                 # 其他参数直接使用
#                 if isinstance(v, str):
#                     init_args.append(f"{k}={repr(v)}")
#                 else:
#                     init_args.append(f"{k}={v}")
#         init_args.insert(0, 'llm_serving=self.llm_serving') #前端没传这个，直接塞进来；
#         init_line = f"self.{var_name} = {cls.__name__}({', '.join(init_args)})"
#         init_lines.append(init_line)

#         # -------- 渲染 run() 调用参数 --------
#         run_args = []
#         if run_has_storage:
#             run_args.append("storage=self.storage.step()")
        
#         for k, v in custom_run_params.items():
#             if isinstance(v, str):
#                 run_args.append(f"{k}={repr(v)}")
#             else:
#                 run_args.append(f"{k}={v}")

#         if run_args:
#             separator = ',\n            '
#             call = (
#                 f"self.{var_name}.run(\n"
#                 f"            {separator.join(run_args)}\n"
#                 f"        )"
#             )
#         else:
#             call = f"self.{var_name}.run()"
#         forward_lines.append(call)

#     return "\n        ".join(init_lines), "\n        ".join(forward_lines)

def render_operator_blocks_with_full_params(
    opname_and_params: List[Dict[str, Any]], 
    op_classes: Dict[str, type],
    prompted_generator_prompts: Optional[Dict[int, str]] = None  # ← 添加参数
) -> tuple:
    """
    渲染算子初始化和调用代码（完整支持 init + run 参数）
    """
    import inspect
    
    init_lines = []
    forward_lines = []
    name_count: Dict[str, int] = {}
    prompted_gen_counter = 0 

    for idx, item in enumerate(opname_and_params):  # ← 使用 enumerate 获取索引
        name = item["op_name"]
        # 支持两种格式：1) init_params/run_params 分离  2) 统一的 params
        custom_init_params = item.get("init_params", {})
        custom_run_params = item.get("run_params", {})
        
        # 如果没有 init_params/run_params，尝试从 params 中获取
        if not custom_init_params and not custom_run_params:
            all_params = item.get("params", {})
            # 根据算子的 __init__ 和 run 签名自动分配参数
            cls = op_classes[name]
            init_param_names = set()
            run_param_names = set()
            
            try:
                init_sig = inspect.signature(cls.__init__)
                init_param_names = {p.name for p in list(init_sig.parameters.values())[1:] 
                                   if p.kind not in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD)}
            except:
                pass
            
            try:
                run_sig = inspect.signature(cls.run)
                run_param_names = {p.name for p in list(run_sig.parameters.values())[1:] 
                                  if p.kind not in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD)
                                  and p.name != "storage"}
            except:
                pass
            
            # 分配参数
            for k, v in all_params.items():
                if k in init_param_names:
                    custom_init_params[k] = v
                elif k in run_param_names:
                    custom_run_params[k] = v
                else:
                    # 默认放到 run_params
                    custom_run_params[k] = v
        
        cls = op_classes[name]
        base_var_name = snake_case(cls.__name__)
        count = name_count.get(base_var_name, 0) + 1
        name_count[base_var_name] = count
        if count > 1:
            var_name = f"{base_var_name}_{count}"
        else:
            var_name = base_var_name

        # 检查 run 方法是否有 storage 参数
        run_has_storage = False
        if hasattr(cls, "run"):
            try:
                run_sig = inspect.signature(cls.run)
                run_has_storage = "storage" in run_sig.parameters
            except:
                pass

        # -------- 渲染 __init__ 参数 --------
        init_args = []
        
        # 检查是否是 PromptedGenerator 且有预生成的 prompt
        if name == "PromptedGenerator" and prompted_generator_prompts:
            if prompted_gen_counter in prompted_generator_prompts:
                pre_prompt = prompted_generator_prompts[prompted_gen_counter]
                custom_init_params["system_prompt"] = pre_prompt
                log.info(f"[render_operator_blocks_with_full_params] 使用预生成的 prompt (索引 {prompted_gen_counter})")
        
        for k, v in custom_init_params.items():
            if k == "llm_serving":
                init_args.append(f"{k}=self.llm_serving")
            elif k == "system_prompt":
                init_args.append(f"{k}={repr(v)}")
            elif k == "prompt_template":
                if v and v != "None":
                    parts = v.rsplit(".", 1)
                    if len(parts) == 2:
                        module, classname = parts
                        EXTRA_IMPORTS.add(f"from {module} import {classname}")
                        init_args.append(f"{k}={classname}()")
                    else:
                        init_args.append(f"{k}={v}")
                else:
                    init_args.append(f"{k}=None")
            else:
                if isinstance(v, str):
                    init_args.append(f"{k}={repr(v)}")
                else:
                    init_args.append(f"{k}={v}")
        
        # ← 如果是 PromptedGenerator，递增计数器
        if name == "PromptedGenerator":
            prompted_gen_counter += 1
        
        # 只在算子的 __init__ 签名中包含 llm_serving 且未在 custom_init_params 中提供时才添加
        try:
            init_sig = inspect.signature(cls.__init__)
            if 'llm_serving' in init_sig.parameters and 'llm_serving' not in custom_init_params:
                init_args.insert(0, 'llm_serving=self.llm_serving')
        except Exception as e:
            log.warning(f"[render_operator_blocks_with_full_params] Failed to inspect {cls.__name__}: {e}")
        init_line = f"self.{var_name} = {cls.__name__}({', '.join(init_args)})"
        init_lines.append(init_line)

        # -------- 渲染 run() 调用参数 --------
        run_args = []
        if run_has_storage:
            run_args.append("storage=self.storage.step()")

        for k, v in custom_run_params.items():
            # 跳过 storage 参数，因为已经在上面自动添加了
            if k == "storage":
                continue
            if isinstance(v, str):
                run_args.append(f"{k}={repr(v)}")
            else:
                run_args.append(f"{k}={v}")

        if run_args:
            separator = ',\n            '
            call = (
                f"self.{var_name}.run(\n"
                f"            {separator.join(run_args)}\n"
                f"        )"
            )
        else:
            call = f"self.{var_name}.run()"
        forward_lines.append(call)

    return "\n        ".join(init_lines), "\n        ".join(forward_lines)


def build_pipeline_code_with_full_params(
    opname_and_params: List[Dict[str, Any]],
    *,
    cache_dir: str = "./cache_local",
    llm_local: bool = False,
    local_model_path: str = "",
    chat_api_url: str = "",
    model_name: str = "gpt-4o",
    file_path: str = "",
    prompted_generator_prompts: Optional[Dict[int, str]] = None,
) -> str:
    """构建完整的 pipeline 代码（支持 init + run 参数）"""
    log.critical(f'[build_pipeline_code_with_full_params]: {chat_api_url}')
    # 清空之前的额外导入
    EXTRA_IMPORTS.clear()
    
    # 1) 提取所有算子名称
    op_names = [item["op_name"] for item in opname_and_params]
    
    # 2) 判断 cache_type
    file_suffix = Path(file_path).suffix.lower() if file_path else ""
    cache_type = {
        ".jsonl": "jsonl",
        ".json": "json",
        ".csv": "csv"
    }.get(file_suffix, "jsonl")

    # 3) 收集导入
    import_lines, op_classes = group_import_for_full_params(op_names)

    # 4) 渲染算子代码（传入 prompted_generator_prompts）
    ops_init_block, forward_block = render_operator_blocks_with_full_params(
        opname_and_params, op_classes, prompted_generator_prompts=prompted_generator_prompts
    )

    # 汇总所有导入语句，去重排序
    all_imports = import_lines + sorted(EXTRA_IMPORTS)
    import_section = "\n".join(dict.fromkeys(all_imports)) 

    # 5) LLM Serving（生成无缩进的代码块）
    if llm_local:
        llm_block = f'''# -------- LLM Serving (Local) --------
self.llm_serving = LocalModelLLMServing_vllm(
    hf_model_name_or_path="{local_model_path}",
    vllm_tensor_parallel_size=1,
    vllm_max_tokens=8192,
    hf_local_dir="local",
    model_name="{model_name}",
)'''
    else:
        llm_block = f'''# -------- LLM Serving (Remote) --------
self.llm_serving = APILLMServing_request(
    api_url="{chat_api_url}chat/completions",
    key_name_of_api_key="DF_API_KEY",
    model_name="{model_name}",
    max_workers=100,
)'''
    log.critical(f'[build_pipeline_code_with_full_params]: {chat_api_url}')
    
    # 处理缩进问题
    llm_block = indent_block(llm_block, 8)
    
    # 6) 模板
    template = '''"""
Auto-generated Pipeline (supports init + run params)
"""
from dataflow.pipeline import PipelineABC
from dataflow.utils.storage import FileStorage
from dataflow.serving import APILLMServing_request, LocalModelLLMServing_vllm

{import_section}

class RecommendPipeline(PipelineABC):
    def __init__(self):
        super().__init__()
        # -------- FileStorage --------
        self.storage = FileStorage(
            first_entry_file_name="{file_path}",
            cache_path="{cache_dir}",
            file_name_prefix="dataflow_cache_step",
            cache_type="{cache_type}",
        )
        {llm_block}

        # -------- Operators --------
        {ops_init_block}

    def forward(self):
        {forward_block}

if __name__ == "__main__":
    pipeline = RecommendPipeline()
    pipeline.compile()
    pipeline.forward()
'''

    code = template.format(
        file_path=file_path,
        import_section=import_section,
        cache_dir=cache_dir,
        cache_type=cache_type, 
        llm_block=llm_block,
        ops_init_block=ops_init_block,
        forward_block=forward_block,
    )
    
    return code



# =========================================================只渲染run函数，其余不管：==================================================
def render_operator_blocks_with_params(
    opname_and_params: List[Dict[str, Any]], 
    op_classes: Dict[str, type], 
    state: DFState
) -> Tuple[str, str]:
    """
    渲染算子初始化和调用代码，支持自定义 run 函数参数
    
    Args:
        opname_and_params: 算子名称和参数列表，格式: [{"op_name": "xxx", "params": {...}}, ...]
        op_classes: 算子类字典
        state: DFState 对象
    
    Returns:
        (初始化代码块, forward调用代码块)
    """
    init_lines: List[str] = []
    forward_lines: List[str] = []
    # 记录同一算子类出现的次数，避免 self.xxx 被后面的覆盖
    name_count: Dict[str, int] = {}

    for item in opname_and_params:
        name = item["op_name"]
        custom_params = item.get("params", {})  # 获取自定义的 run 参数
        
        cls = op_classes[name]
        base_var_name = snake_case(cls.__name__)
        count = name_count.get(base_var_name, 0) + 1
        name_count[base_var_name] = count
        if count > 1:
            var_name = f"{base_var_name}_{count}"
        else:
            var_name = base_var_name

        init_kwargs, run_kwargs, run_has_storage = extract_op_params(cls)

        # -------- 渲染 __init__ 参数 --------
        rendered_init_args: List[str] = []
        for k, v in init_kwargs:
            if k == "llm_serving":
                rendered_init_args.append(f"{k}=self.llm_serving")
            elif k == "prompt_template":
                p_t = choose_prompt_template_by_llm(name, state)
                rendered_init_args.append(f'{k}={p_t}')
            else:
                rendered_init_args.append(f"{k}={v}")

        init_line = f"self.{var_name} = {cls.__name__}(" + ", ".join(rendered_init_args) + ")"
        init_lines.append(init_line)

        # -------- 渲染 run() 调用参数 --------
        run_args: List[str] = []
        
        # 第一个参数 storage 保持不变
        if run_has_storage:
            run_args.append("storage=self.storage.step()")
        
        # 处理其他参数：优先使用自定义参数，否则使用默认值
        for k, default_v in run_kwargs:
            if k in custom_params:
                # 使用自定义参数值，需要正确格式化
                actual_value = custom_params[k]
                if isinstance(actual_value, str):
                    formatted_value = repr(actual_value)
                elif isinstance(actual_value, (int, float, bool, type(None))):
                    formatted_value = repr(actual_value)
                elif isinstance(actual_value, (list, dict)):
                    formatted_value = repr(actual_value)
                else:
                    # 其他类型尝试转字符串
                    formatted_value = repr(actual_value)
                run_args.append(f"{k}={formatted_value}")
            else:
                # 使用默认值
                run_args.append(f"{k}={default_v}")

        # 构建完整的 run 调用
        if run_args:
            call = (
                f"self.{var_name}.run(\n"
                f"    " + ",\n    ".join(run_args) + "\n"
                f")"
            )
        else:
            call = f"self.{var_name}.run()"
        forward_lines.append(call)

    return "\n".join(init_lines), "\n".join(forward_lines)


def build_pipeline_code_with_run_params(
    opname_and_params: List[Dict[str, Any]],
    state: DFState,
    *,
    cache_dir: str = "./cache_local",
    llm_local: bool = False,
    local_model_path: str = "",
    chat_api_url: str = "",
    model_name: str = "gpt-4o",
    file_path: str = "",
    prompted_generator_prompts: Optional[Dict[int, str]] = None 
) -> str:
    """
    构建 pipeline 代码，支持为每个算子指定 run 函数的实际参数

    注意：
    - render_operator_blocks_with_full_params 返回的代码已经包含了内部缩进
      （使用 "\n        ".join()），因此不需要再次使用 indent_block
    
    Args:
        opname_and_params: 算子名称和参数列表
            格式: [
                {"op_name": "OperatorA", "params": {"param1": "value1", "param2": 123}},
                {"op_name": "OperatorB", "params": {"param_x": True}},
                ...
            ]
            其中 params 是该算子 run 函数的参数（不包括 storage）
        state: DFState 对象
        cache_dir: 缓存目录
        llm_local: 是否使用本地 LLM
        local_model_path: 本地模型路径
        chat_api_url: API URL
        model_name: 模型名称
        file_path: 输入文件路径
        prompted_generator_prompts: 预生成的 PromptedGenerator system_prompt 映射
    
    Returns:
        生成的 pipeline 代码字符串
    """
    # 1) 提取所有算子名称
    op_names = [item["op_name"] for item in opname_and_params]
    
    # 2) 根据 file_path 后缀判断 cache_type
    file_suffix = Path(file_path).suffix.lower() if file_path else ""
    if file_suffix == ".jsonl":
        cache_type = "jsonl"
    elif file_suffix == ".json":
        cache_type = "json"
    elif file_suffix == ".csv":
        cache_type = "csv"  
    else:
        cache_type = "jsonl" 
        log.warning(f"[pipeline_assembler] Unknown file suffix '{file_suffix}', defaulting to 'jsonl'")

    # 3) 收集导入与类
    import_lines, stub_blocks, op_classes = group_imports(op_names)

    # 4) 渲染 operator 代码片段
    # render_operator_blocks_with_full_params 返回的代码已经包含了正确的缩进
    ops_init_block, forward_block = render_operator_blocks_with_full_params(
        opname_and_params, 
        op_classes,
        prompted_generator_prompts=prompted_generator_prompts
    )

    import_lines.extend(sorted(EXTRA_IMPORTS))
    
    import_section = "\n".join(import_lines)
    stub_section = "\n\n".join(stub_blocks)

    # 5) LLM-Serving 片段（无缩进，统一在模板中缩进）
    if llm_local:
        llm_block_raw = f"""
# -------- LLM Serving (Local) --------
self.llm_serving = LocalModelLLMServing_vllm(
    hf_model_name_or_path="{local_model_path}",
    vllm_tensor_parallel_size=1,
    vllm_max_tokens=8192,
    hf_local_dir="local",
    model_name="{model_name}",
)
"""
    else:
        llm_block_raw = f"""
# -------- LLM Serving (Remote) --------
self.llm_serving = APILLMServing_request(
    api_url="{chat_api_url}chat/completions",
    key_name_of_api_key="DF_API_KEY",
    model_name="{model_name}",
    max_workers=100,
)
"""

    # 6) 统一缩进 llm_block
    llm_block = indent_block(llm_block_raw, 8)

    # 7) 模板
    template = '''"""
Auto-generated by pipeline_assembler (with custom run params)
"""
from dataflow.pipeline import PipelineABC
from dataflow.utils.storage import FileStorage
from dataflow.serving import APILLMServing_request, LocalModelLLMServing_vllm

{import_section}

{stub_section}

class RecommendPipeline(PipelineABC):
    def __init__(self):
        super().__init__()
        # -------- FileStorage --------
        self.storage = FileStorage(
            first_entry_file_name="{file_path}",
            cache_path="{cache_dir}",
            file_name_prefix="dataflow_cache_step",
            cache_type="{cache_type}",
        )
{llm_block}
        # -------- Operators --------
        {ops_init_block}

    def forward(self):
        {forward_block}

if __name__ == "__main__":
    pipeline = RecommendPipeline()
    pipeline.compile()
    pipeline.forward()
'''

    # 8) 格式化并返回
    code = template.format(
        file_path=file_path,
        import_section=import_section,
        stub_section=stub_section,
        cache_dir=cache_dir,
        cache_type=cache_type, 
        llm_block=llm_block,
        ops_init_block=ops_init_block,
        forward_block=forward_block,
    )
    return code


def pipeline_assembler_with_params(
    opname_and_params: List[Dict[str, Any]], 
    state: DFState,
    **kwargs
) -> Dict[str, Any]:
    """
    Pipeline 组装器（支持自定义 run 参数版本）
    
    Args:
        opname_and_params: 算子名称和参数列表
            格式: [{"op_name": "xxx", "params": {...}}, ...]
        state: DFState 对象
        **kwargs: 其他参数传递给 build_pipeline_code_with_run_params
    
    Returns:
        包含生成代码的字典 {"pipe_code": ...}
    """
    code = build_pipeline_code_with_run_params(opname_and_params, state, **kwargs)
    return {"pipe_code": code}


async def apipeline_assembler_with_params(
    opname_and_params: List[Dict[str, Any]], 
    state: DFState,
    **kwargs
) -> Dict[str, Any]:
    """异步版本"""
    return pipeline_assembler_with_params(opname_and_params, state, **kwargs)


# ================================之前的版本
def build_pipeline_code(
    op_names: List[str],
    state: DFState,
    *,
    cache_dir: str = "./cache_local",
    llm_local: bool = False,
    local_model_path: str = "",
    chat_api_url: str = "",
    model_name: str = "gpt-4o",
    file_path: str = "",
    prompted_generator_prompts: Optional[Dict[int, str]] = None,  # ← 添加这个参数
) -> str:
    # 1) 根据 file_path 后缀判断 cache_type
    file_suffix = Path(file_path).suffix.lower() if file_path else ""
    if file_suffix == ".jsonl":
        cache_type = "jsonl"
    elif file_suffix == ".json":
        cache_type = "json"
    elif file_suffix == ".csv":
        cache_type = "csv"  
    else:
        cache_type = "jsonl" 
        log.warning(f"[pipeline_assembler] Unknown file suffix '{file_suffix}', defaulting to 'jsonl'")

    # 2) 收集导入与类
    import_lines, stub_blocks, op_classes = group_imports(op_names)

    # 3) 渲染 operator 代码片段（传递 prompted_generator_prompts）
    ops_init_block_raw, forward_block_raw = render_operator_blocks(
        op_names, 
        op_classes, 
        state,
        prompted_generator_prompts=prompted_generator_prompts
    )

    import_lines.extend(sorted(EXTRA_IMPORTS))
    
    import_section = "\n".join(import_lines)
    stub_section = "\n\n".join(stub_blocks)

    # 4) LLM-Serving 片段（无缩进，统一在模板中缩进）
    if llm_local:
        llm_block_raw = f"""
# -------- LLM Serving (Local) --------
self.llm_serving = LocalModelLLMServing_vllm(
    hf_model_name_or_path="{local_model_path}",
    vllm_tensor_parallel_size=1,
    vllm_max_tokens=8192,
    hf_local_dir="local",
    model_name="{model_name}",
)
"""
    else:
        llm_block_raw = f"""
# -------- LLM Serving (Remote) --------
self.llm_serving = APILLMServing_request(
    api_url="{chat_api_url}chat/completions",
    key_name_of_api_key="DF_API_KEY",
    model_name="{model_name}",
    max_workers=100,
)
"""

    # 5) 统一缩进
    llm_block = indent_block(llm_block_raw, 8)
    ops_init_block = indent_block(ops_init_block_raw, 8)
    forward_block = indent_block(forward_block_raw, 8)

    # 6) 模板
    template = '''"""
Auto-generated by pipeline_assembler
"""
from dataflow.pipeline import PipelineABC
from dataflow.utils.storage import FileStorage
from dataflow.serving import APILLMServing_request, LocalModelLLMServing_vllm

{import_section}

{stub_section}

class RecommendPipeline(PipelineABC):
    def __init__(self):
        super().__init__()
        # -------- FileStorage --------
        self.storage = FileStorage(
            first_entry_file_name="{file_path}",
            cache_path="{cache_dir}",
            file_name_prefix="dataflow_cache_step",
            cache_type="{cache_type}",
        )
{llm_block}

{ops_init_block}

    def forward(self):
{forward_block}

if __name__ == "__main__":
    pipeline = RecommendPipeline()
    pipeline.compile()
    pipeline.forward()
'''

    # 7) 格式化并返回
    code = template.format(
        file_path=file_path,
        import_section=import_section,
        stub_section=stub_section,
        cache_dir=cache_dir,
        cache_type=cache_type, 
        llm_block=llm_block,
        ops_init_block=ops_init_block,
        forward_block=forward_block,
    )
    return code


def pipeline_assembler(recommendation: List[str], state: DFState,**kwargs) -> Dict[str, Any]:
    code = build_pipeline_code(recommendation, state, **kwargs)
    return {"pipe_code": code}


async def apipeline_assembler(recommendation: List[str], **kwargs) -> Dict[str, Any]:
    return pipeline_assembler(recommendation, **kwargs)

# ===================================================================通过my pipline的 py文件，拿到结构化的输出信息
"""
Parse a generated PipelineABC python file and export a graph schema::

    {
      "nodes": [...],
      "edges": [...]
    }

Requirements:
    - 支持 input_key / output_key 既可以是关键字参数也可以是位置参数
    - 允许同一个算子 run 多次
    - nodes.id 直接使用 self.xxx 的变量名
"""
from collections import defaultdict
from dataflow.utils.registry import OPERATOR_REGISTRY

# ----------------------------------------------------- #
# config & helpers
# ----------------------------------------------------- #
SKIP_CLASSES: set[str] = {
    "FileStorage",
    "APILLMServing_request",
    "LocalModelLLMServing_vllm",
}

_IN_PREFIXES = ("input", "input_")
_OUT_PREFIXES = ("output", "output_")


def _is_input(name: str) -> bool:
    return name.startswith(_IN_PREFIXES)


def _is_output(name: str) -> bool:
    return name.startswith(_OUT_PREFIXES)


def _guess_type(cls_obj: type | None, cls_name: str) -> str:
    """
    Guess operator category for front-end icon & color.
    规则:
        1. package 名倒数第二段 (operators.xxx.{filter|parser}.xxx)
        2. 类名后缀启发
        3. 兜底 'other'
    """
    # rule-1
    if cls_obj is not None:
        parts = cls_obj.__module__.split(".")
        if len(parts) >= 2:
            candidate = parts[-2]
            if candidate not in {"__init__", "__main__"}:
                return candidate
    # rule-2
    lower = cls_name.lower()
    for suf, cat in [
        ("parser", "parser"),
        ("generator", "generate"),
        ("filter", "filter"),
        ("evaluator", "eval"),
        ("refiner", "refine"),
    ]:
        if lower.endswith(suf):
            return cat
    # rule-3
    return "other"


def _literal_eval_safe(node: ast.AST) -> Any:
    """ast.literal_eval 的宽松版本，失败就返回反编译字符串"""
    if isinstance(node, ast.Constant):  # fast path
        return node.value
    try:
        return ast.literal_eval(node)
    except Exception:
        return ast.unparse(node) if hasattr(ast, "unparse") else repr(node)


# ----------------------------------------------------- #
# AST 解析主流程
# ----------------------------------------------------- #
def parse_pipeline_file(file_path: str | Path) -> Dict[str, Any]:
    """
    Parameters
    ----------
    file_path : str | Path
        生成的 pipeline python 文件路径

    Returns
    -------
    dict
        {"nodes": [...], "edges": [...]}
    """
    file_path = Path(file_path)
    src = file_path.read_text(encoding="utf-8")
    tree = ast.parse(src, filename=str(file_path))

    # ------------------------------------------------- #
    # 1. 解析 __init__ 里的 operator 实例
    # ------------------------------------------------- #
    def _parse_init(init_func: ast.FunctionDef) -> Dict[str, Tuple[str, Dict[str, Any]]]:
        """
        Returns
        -------
        var_name -> (cls_name, init_kwargs)
        """
        results: Dict[str, Tuple[str, Dict[str, Any]]] = {}
        for stmt in init_func.body:
            if (
                isinstance(stmt, ast.Assign)
                and stmt.targets
                and isinstance(stmt.targets[0], ast.Attribute)
                and isinstance(stmt.value, ast.Call)
            ):
                attr: ast.Attribute = stmt.targets[0]
                if not (isinstance(attr.value, ast.Name) and attr.value.id == "self"):
                    continue
                var_name = attr.attr

                call: ast.Call = stmt.value
                # 取类名
                if isinstance(call.func, ast.Name):
                    cls_name = call.func.id
                elif isinstance(call.func, ast.Attribute):
                    cls_name = call.func.attr
                else:
                    continue

                if cls_name in SKIP_CLASSES:  # 跳过非算子
                    continue

                kwargs = {
                    kw.arg: _literal_eval_safe(kw.value)
                    for kw in call.keywords
                    if kw.arg is not None
                }
                results[var_name] = (cls_name, kwargs)
        return results

    # ------------------------------------------------- #
    # 2. 解析 forward() 里的 run 调用
    # ------------------------------------------------- #
    def _parse_forward(
        forward_func: ast.FunctionDef,
    ) -> DefaultDict[str, List[Dict[str, Any]]]:
        """
        Returns
        -------
        var_name -> [run_kwargs ...]  (保持出现顺序)
        """
        mapping: DefaultDict[str, List[Dict[str, Any]]] = defaultdict(list)

        # walk 按源码顺序遍历需借助 ast.iter_child_nodes + 递归
        def _visit(node: ast.AST):
            # 按出现顺序遍历
            for child in ast.iter_child_nodes(node):
                if (
                    isinstance(child, ast.Call)
                    and isinstance(child.func, ast.Attribute)
                    and child.func.attr == "run"
                ):
                    obj = child.func.value
                    if (
                        isinstance(obj, ast.Attribute)
                        and isinstance(obj.value, ast.Name)
                        and obj.value.id == "self"
                    ):
                        var_name = obj.attr

                        # ------- 关键字参数 -------
                        kw_dict = {
                            kw.arg: _literal_eval_safe(kw.value)
                            for kw in child.keywords
                            if kw.arg is not None
                        }

                        # ------- 位置参数 -------
                        # 假设位置顺序为 (storage, input_key, output_key, ...)
                        if len(child.args) >= 2:
                            kw_dict.setdefault("input_key", _literal_eval_safe(child.args[1]))
                        if len(child.args) >= 3:
                            kw_dict.setdefault("output_key", _literal_eval_safe(child.args[2]))

                        mapping[var_name].append(kw_dict)
                _visit(child)

        _visit(forward_func)
        return mapping

    # ------------------------------------------------- #
    # 3. 主 visitor：定位唯一继承 PipelineABC 的类
    # ------------------------------------------------- #
    init_ops, forward_calls = {}, defaultdict(list)

    class PipelineVisitor(ast.NodeVisitor):
        def visit_ClassDef(self, node: ast.ClassDef):  # noqa: N802
            nonlocal init_ops, forward_calls
            # naive 判断: 存在 forward() 方法即认为是 pipeline
            has_forward = any(
                isinstance(b, ast.FunctionDef) and b.name == "forward" for b in node.body
            )
            if not has_forward:
                return
            for item in node.body:
                if isinstance(item, ast.FunctionDef):
                    if item.name == "__init__":
                        init_ops = _parse_init(item)
                    elif item.name == "forward":
                        forward_calls = _parse_forward(item)

    PipelineVisitor().visit(tree)

    # ------------------------------------------------- #
    # 4. build nodes
    # ------------------------------------------------- #
    def build_nodes() -> tuple[list[dict[str, Any]],
                            dict[str, str],
                            dict[str, tuple[str, str]]]:
        """
        Returns
        -------
        nodes                : list of node-dict
        var2id               : var_name -> node_id        (供后续查表)
        produced_ports       : label(str) -> (node_id, port_name)
        """
        nodes: list[dict[str, Any]] = []
        var2id: dict[str, str] = {}
        produced_ports: dict[str, tuple[str, str]] = {}

        global_counter = itertools.count(1)       

        for var, (cls_name, init_kwargs) in init_ops.items():
            # -------- 生成 node_id -------- #
            node_id = f"node{next(global_counter)}"    # <-- 变成 node1/node2/…

            var2id[var] = node_id

            # forward() 第一次 run 的配置
            first_run_cfg = forward_calls.get(var, [{}])[0]

            # 把首次 run 产生的 output 标记为 “已经产生”
            for k, v in first_run_cfg.items():
                if _is_output(k) and isinstance(v, str):
                    produced_ports[v] = (node_id, k)
            try:
                cls_obj = OPERATOR_REGISTRY.get(cls_name)
            except Exception:
                cls_obj = None

            nodes.append(
                {
                    "id": node_id,
                    "name": cls_name,
                    "type": _guess_type(cls_obj, cls_name),
                    "config": {
                        "init": init_kwargs,
                        "run": first_run_cfg,
                    },
                }
            )
        return nodes, var2id, produced_ports

    # ------------------------------------------------- #
    # 5. build edges (按 forward 执行顺序)
    # ------------------------------------------------- #
    def build_edges(
        produced_ports: dict[str, tuple[str, str]],
        var2id: dict[str, str],
    ) -> list[dict[str, Any]]:
        edges: list[dict[str, Any]] = []
        for var, runs in forward_calls.items():
            tgt_id = var2id.get(var)
            if not tgt_id:
                continue
            for run_cfg in runs:
                for k, v in run_cfg.items():
                    if _is_input(k) and isinstance(v, str) and v in produced_ports:
                        src_id, src_port = produced_ports[v]
                        edges.append(
                            {
                                "source": src_id,
                                "target": tgt_id,
                                "source_port": src_port,
                                "target_port": k,
                            }
                        )
        return edges

    nodes, var2id, produced_ports = build_nodes()
    edges = build_edges(produced_ports, var2id)
    return {"nodes": nodes, "edges": edges}

def build_edges_from_nodes(
    nodes: List[Dict[str, Any]] | Dict[str, Any],
    save_path: str | Path
) -> Dict[str, Any]:
    """
    根据 nodes 自动生成 edges，并保存完整的 pipeline graph 到 save_path

    Args:
        nodes: 节点信息，支持两种格式：
               - List[Dict]: 直接的节点列表
               - Dict: 包含 "nodes" 键的字典，如 {"nodes": [...]}
        save_path: 输出 json 文件路径

    Returns:
        完整的 graph dict: {"nodes": ..., "edges": ...}
    
    Raises:
        ValueError: 当输入格式不正确时
    """
    
    # 统一处理输入格式，提取节点列表
    if isinstance(nodes, dict):
        if "nodes" in nodes:
            nodes_list = nodes["nodes"]
        else:
            raise ValueError("当 nodes 为字典时，必须包含 'nodes' 键")
    elif isinstance(nodes, list):
        nodes_list = nodes
    else:
        raise ValueError(f"nodes 必须是 list 或 dict 类型，当前类型: {type(nodes)}")
    
    # 验证节点列表不为空
    if not nodes_list:
        log.warning("[build_edges_from_nodes] 节点列表为空")
        graph = {"nodes": [], "edges": []}
        save_path = Path(save_path)
        save_path.write_text(json.dumps(graph, indent=2, ensure_ascii=False), encoding="utf-8")
        return graph

    # 1. 收集所有 output_key 到 (node_id, port_name) 的映射
    produced_outputs = {}  # output_key_value -> (node_id, port_name)
    for node in nodes_list:
        if not isinstance(node, dict) or "id" not in node:
            log.warning(f"[build_edges_from_nodes] 跳过无效节点: {node}")
            continue
            
        run_cfg = node.get("config", {}).get("run", {})
        for key, value in run_cfg.items():
            if isinstance(key, str) and key.startswith("output") and isinstance(value, str):
                produced_outputs[value] = (node["id"], key)

    # 2. 遍历节点，查找 input_key 引用的 output_key，生成边
    edges = []
    for node in nodes_list:
        if not isinstance(node, dict) or "id" not in node:
            continue
            
        run_cfg = node.get("config", {}).get("run", {})
        for key, value in run_cfg.items():
            if isinstance(key, str) and key.startswith("input") and isinstance(value, str):
                if value in produced_outputs:
                    src_id, src_port = produced_outputs[value]
                    edges.append({
                        "source": src_id,
                        "target": node["id"],
                        "source_port": src_port,
                        "target_port": key
                    })

    # 3. 保存并返回
    graph = {"nodes": nodes_list, "edges": edges}
    save_path = Path(save_path)
    save_path.write_text(json.dumps(graph, indent=2, ensure_ascii=False), encoding="utf-8")
    
    log.info(f"[build_edges_from_nodes] 生成 {len(nodes_list)} 个节点，{len(edges)} 条边")
    return graph

# ----------------------------------------------------- #
# CLI 方便快速测试（免参数版）
# ----------------------------------------------------- #
if __name__ == "__main__":
    import json
    from pathlib import Path
    import pprint

    PY_PATH = Path("")

    graph = parse_pipeline_file(PY_PATH)

    pprint.pprint(graph, width=120)

    OUT_PATH = PY_PATH.with_suffix(".json")
    OUT_PATH.write_text(json.dumps(graph, indent=2, ensure_ascii=False), encoding="utf-8")
    print(f"saved to {OUT_PATH}")





















# if __name__ == "__main__":
    # test_ops = [
    #     "SQLGenerator",
    #     "SQLExecutionFilter",
    #     "SQLComponentClassifier",
    # ]
    # result = pipeline_assembler(
    #     test_ops,
    #     cache_dir="./cache_local",
    #     llm_local=False,
    #     chat_api_url="",
    #     model_name="gpt-4o",
    #     file_path = " "
    # )
    # code_str = result["pipe_code"]
    # write_pipeline_file(code_str, file_name="my_recommend_pipeline.py", overwrite=True)
    # print("Generated pipeline code written to my_recommend_pipeline.py")
